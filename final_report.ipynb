{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea1fa39-ab28-4cdd-8079-e7638270f42a",
   "metadata": {},
   "source": [
    "# THE LANGUAGE of LIFE EXPECTANCY: \n",
    "\n",
    "#### A Natural Language Processing Evaluation of GitHub Repository Content Programming Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9b74c-6b47-44c6-a107-c514197e6e4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608c6c5-e218-4c88-a9fe-00589976a771",
   "metadata": {},
   "source": [
    "**Natural Language Processing Project & Final Report Created By:**  \n",
    "\n",
    "Chris Teceno, Rachel Robbins-Mayhill, Kristofer Rivera     \n",
    "  Codeup     **|**     Innis Cohort     **|**     May 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2f652-03d9-463f-83b5-cad83a99833c",
   "metadata": {},
   "source": [
    "<img src='languages.png' width=\"1500\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d56e3-72e6-4f11-b805-bafc985ada84",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "The goal of this project is to build a Natural Language Processing (NLP) model that can predict the programming language of projects within specified GitHub repositories, given the text of a README.md file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac48c5-edc3-4d86-a51a-cbab608764da",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "This project was initiated by utilizing web scraping techniques to scrape README files from specified GitHub repositories focused on Life Expectancy projects. The 130 most starred Life Expectancy Repositories were used as the documents within the corpus for this NLP project. \n",
    "\n",
    "After acquiring and preparing the corpus, our team conducted natural language processing exploration methods such as word clouds, bigrams and trigrams. We employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the programming language used in a GitHub repository based on the words and word combinations found in the readme files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c1257-dbb7-4988-9b49-dbb075f6d9aa",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d0f57-14a5-4c70-9f9e-53e586e6e140",
   "metadata": {},
   "source": [
    "## Initial Questions:\n",
    "Data-Focused Questions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554f95a-4355-4ae8-a485-c8fc991df4d4",
   "metadata": {},
   "source": [
    "## EXECUTIVE SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b4088-318d-4003-835b-2db459130f6e",
   "metadata": {},
   "source": [
    "==============================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a858d-58a9-46f3-b8a1-3ea7f5c56652",
   "metadata": {},
   "source": [
    "## I. ACQUIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef3e24-6c5e-4ca4-aecb-5320514bc342",
   "metadata": {},
   "source": [
    "### Note about imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b82a20-3e3d-48c9-b6e6-c7054113b585",
   "metadata": {},
   "source": [
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34e1ade-c133-4eb2-88c3-d951b92107ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import wrangle\n",
    "from env import github_token, github_username\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0001cf-72d7-4ca9-ac86-14ce8776215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mcastrolab/Brazil-Covid19-e0-change</td>\n",
       "      <td>R</td>\n",
       "      <td># Reduction in life expectancy in Brazil after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jschoeley/de0anim</td>\n",
       "      <td>R</td>\n",
       "      <td># Animated annual changes in life-expectancy\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sychi77/Thoracic_Surgery_Patient_Survival</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td># Thoracic Surgery for Lung Cancer Data Set\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ashtad63/HackerRank-Data-Scientist-Hiring-Test</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td># HackerRank Data Scientist Hiring Test: Predi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OxfordDemSci/ex2020</td>\n",
       "      <td>R</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;img src=\"https://github...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             repo          language  \\\n",
       "0             mcastrolab/Brazil-Covid19-e0-change                 R   \n",
       "1                               jschoeley/de0anim                 R   \n",
       "2       sychi77/Thoracic_Surgery_Patient_Survival  Jupyter Notebook   \n",
       "3  ashtad63/HackerRank-Data-Scientist-Hiring-Test  Jupyter Notebook   \n",
       "4                             OxfordDemSci/ex2020                 R   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # Reduction in life expectancy in Brazil after...  \n",
       "1  # Animated annual changes in life-expectancy\\n...  \n",
       "2  # Thoracic Surgery for Lung Cancer Data Set\\n ...  \n",
       "3  # HackerRank Data Scientist Hiring Test: Predi...  \n",
       "4  <p align=\"center\">\\n  <img src=\"https://github...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acquire data from .json saved and processed using functions found in wrangle.py\n",
    "df = pd.read_json(\"data.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ae14e9-26fa-4547-950c-1a1590502dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64848218-ea03-4067-8c3d-a65e32bffdd5",
   "metadata": {},
   "source": [
    "## The Original DataFrame Size: 150,000 rows, or observations, and 12 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0bfbe-994a-4def-9f39-301eac61eb01",
   "metadata": {},
   "source": [
    "================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b50157-99f6-4539-9489-bdb1387b171f",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b790cd2-4448-4951-ae63-ce33de70545c",
   "metadata": {},
   "source": [
    "After data acquisition, the table was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the ____________  function saved within the wrangle.py file inside the 'NLP-Project' repository on GitHub. The function takes in the original data.json dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Delete \"Unnamed' index\n",
    "- Rename columns for understanding, while making lowercase for ease of understanding and coding through exploration\n",
    "- Drop missing values (29_731 in monthly_income and 3_924 in quantity_dep) to prevent impediments in exploration and modeling \n",
    "- Create categorical columns for binning age and dependents in order to visualize and identify relationships more easily in exploration\n",
    "- Manually scaled monthly income to include only those with monthly income below\\\\$15,000 to eliminate outliers\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "The missing value removal equated to removing     observations, which was about ___\\% of the data set. It still left a substantial number of observations, above 1100. If given more time with the data, it is recommended to _______."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913212cd-4acf-4a42-b6b5-2afac70d3e12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca494ea-7dd3-44e0-8278-39dddd9c7d8a",
   "metadata": {},
   "source": [
    "### Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd29ca-232d-45af-b3db-81a53e3a26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the wrangle_client function found in the wrangle.py\n",
    "df = wrangle.___________(df)\n",
    "# view first few rows of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64fd9a-29db-49dd-acc9-f907b1b52a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the number of rows and columns for the updated/cleaned dataframe. \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d254fef-dc05-4834-ad75-09ef95a6cd8d",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: 120,269 rows, 11 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35601-337e-41fd-b855-a64d0ccb3b0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a12d7-6226-4262-ab57-542407d5f3d5",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f395b-4e5c-426e-b786-1642b49955a0",
   "metadata": {},
   "source": [
    "After preparing the data, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: 42\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the wrangle.py file inside the 'NLP-Project' repository on GitHub, located [here](https://github.com/Two-Guys-and-a-Gal/NLP-Project) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403163e-d21d-4f0e-a98f-bc599c2108b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the wrangle.py\n",
    "train, validate, test = wrangle.split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6dd627-91d8-45cb-a096-5e87b3463862",
   "metadata": {},
   "source": [
    "================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25035c38-01ab-4765-b15b-cce6ef997567",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ed45d-7b2d-4f13-a6ef-11b8de9aac86",
   "metadata": {},
   "source": [
    "After acquiring and preparing the data, exploration was conducted. All univariate exploration was completed on the entire cleaned dataset in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the data was split into train, validate, and test samples, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c095f8f-5615-46ef-b049-61d09f5bde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0123f5-0e68-461a-8004-3a15a886f531",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1858aec-739e-4642-9da8-b432d3b117b4",
   "metadata": {},
   "source": [
    "### UNIVARIATE EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461b779-bfe8-4746-a988-76637075afc4",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION of TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa2919-5a17-437c-bc9b-e4cc0b609ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain counts of target variable and display them through print statement\n",
    "print(df.serious_delinquency.value_counts())\n",
    "\n",
    "\n",
    "# plot figure size\n",
    "plt.figure(figsize=(15,9))\n",
    "# define variable to plot\n",
    "y = df.serious_delinquency.value_counts()\n",
    "# format title and font\n",
    "plt.title('Serious Delinquency', fontsize=20)\n",
    "# create pie chart\n",
    "plt.pie(y)\n",
    "plt.show() \n",
    "\n",
    "# obtain percentage breakdown of target variable and round it to the nearest hundredth\n",
    "print(\"Percent Seriously Delinquent from Overall Dataset\")\n",
    "round(df.serious_delinquency.mean(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8241be-aaca-4a85-8d25-dad5013ee49d",
   "metadata": {},
   "source": [
    "**OBSERVATIONS:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99531bed-4991-4525-9ebf-117f92c54c00",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION SUMMARY:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc7981-9fa3-4c43-a5f1-d5ed00aa785e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e691b-536a-4d40-9910-309c0cc2fb2d",
   "metadata": {},
   "source": [
    "### EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871d6ae-6292-4d53-a1b8-a8c5f9ed7dd3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ffeac-d04c-403a-ba2e-e4fac3aa74e3",
   "metadata": {},
   "source": [
    "### EXPLORATION SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309d169-45d2-4f40-8ae0-d08d69c693bd",
   "metadata": {},
   "source": [
    "================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efe8df-7429-49d4-89ee-994d1fc0dee4",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378bc311-08f9-44e8-aaae-4aa3a5879353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for modeling\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0715e08-1ebc-4582-b026-e65dfa037c4c",
   "metadata": {},
   "source": [
    "### MODELING CONFUSION MATRIX\n",
    "- True Positive: number of occurrences where serious delinquency is true and serious delinquency is predicted true.\n",
    "    - protects borrower and institution from granting credit when high-risk (Optimize)\n",
    "- True Negative: number of occurrences where serious delinquency is false and serious delinquency is predicted false.\n",
    "    - allows those who do not have serious delinquency to access credit\n",
    "- False Positive: number of occurrences where serious delinquency is false and serious delinquency is predicted true.\n",
    "    - prevents non-at-risk borrowers from receiving credit, which could impact the borrower and institution negatively, but the instiution less-so\n",
    "- False Negative: number of occurrences where serious delinquency is true and serious delinquency is predicted false.\n",
    "    - allows at-risk borrowers access to credit; this could be the most costly outcome to the borrower and the institution (Optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f99e8-79a4-418e-8c0a-b054fd9ade0e",
   "metadata": {},
   "source": [
    "Because serious delinquency is a yes or no (boolean) value, classification machine learning algorithms were used to fit to the training data and the models were evaluated on validate data. The best model was selected using accuracy, but also with an eye for recall. In other words, the model was optimized for identifying true positives (actual delinquency when predicted), and false negatives (serious delinquency when predicted not to have serious delinquency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5a808-7ccd-4658-a72b-9b741eaf863b",
   "metadata": {},
   "source": [
    "### MODEL - SCALE\n",
    "As previously mentioned, scaling was done manually to monthly income inside the prep function. Monthly income was scaled to include only observations below \\\\$15,000 to be more in line with the typical borrower. By including only the observations noted, 95\\% of the data was still retained. No other scaling was conducted at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6c02e-9a2d-48af-b957-c6c00e53708f",
   "metadata": {},
   "source": [
    "### Set X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788ce32-2712-464c-a3b8-a47b11b33855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X & y version of train, where y is a series with just the target variable and X are all the features. \n",
    "X_train = train.drop(columns=['serious_delinquency','quantity_90_days_pd', 'age_bins', 'quantity_dependents_bins'])\n",
    "y_train = train.serious_delinquency\n",
    "\n",
    "X_validate = validate.drop(columns=['serious_delinquency', 'quantity_90_days_pd', 'age_bins', 'quantity_dependents_bins'])\n",
    "y_validate = validate.serious_delinquency\n",
    "\n",
    "X_test = test.drop(columns=['serious_delinquency', 'quantity_90_days_pd', 'age_bins', 'quantity_dependents_bins'])\n",
    "y_test = test.serious_delinquency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e244ea-52f1-49cc-902a-06e6a02a993e",
   "metadata": {},
   "source": [
    "### Set Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dadaf-8a44-4a49-9a46-6063144cf775",
   "metadata": {},
   "source": [
    "A baseline prediction was set by predicting all repositories will have ____________ as the programming language. We will evaluate the accuracy of our models in comparrison to that baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cff2d-6f0c-447d-92e5-65dfdf56abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the mode for the target\n",
    "baseline = y_train.mode()\n",
    "\n",
    "# produce boolean array with True assigned to match the baseline prediction and real data. \n",
    "matches_baseline_prediction = (y_train == 0)\n",
    "\n",
    "baseline_accuracy = matches_baseline_prediction.mean()\n",
    "\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd542af0-8265-40c2-ad60-cfb607e43d85",
   "metadata": {},
   "source": [
    "#### The 3 models built were \n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "The models were run with many trials, adjusting parameters and algorithms to find the best performing model.  \n",
    "\n",
    "- None of these model appeared to be overfit.\n",
    "- None of the models performed significantly better than baseline.\n",
    "- The Random Forest Model that performed best had 12 samples_per_leaf and max_depth of 12, with train accuracy of 94% and validate accuracy of 93% performing only 1% better than baseline with validate. It was then applied to the un-seen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51ceea-5d8d-4f0b-9762-c4faa8bb59b0",
   "metadata": {},
   "source": [
    "### MODEL - DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d8059-f50a-45b2-8940-76cc2c20c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the object\n",
    "clf1 = DecisionTreeClassifier(max_depth=2, random_state=123)\n",
    "# Fit the model\n",
    "clf1 = clf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f0691-d624-49af-b9b2-2c497b11b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = clf1.predict(X_train)\n",
    "\n",
    "# estimate probability\n",
    "y_pred_proba = clf1.predict_proba(X_train)\n",
    "pd.DataFrame(y_pred_proba, columns = ['Not Seriously Delinquent', 'Seriously Delinquent']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c62ba-0a38-4fad-a65e-ada000436db4",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989b9e2-914e-42e3-8385-a6e26dab8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain accuracy of model\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf1.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd244be-25cf-4e74-b2ea-71d3ebb2c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification report to look at model results\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb0334-56ce-4c9b-8f0d-b3fcf15ae2f2",
   "metadata": {},
   "source": [
    "### Evaluate the Model with our Validate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc977c-0901-4810-9603-929aa52a6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Decision Tree classifier on validate set: {:.2f}'\n",
    "     .format(clf1.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094eea8-28ad-4f61-9bf7-0212c4895035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce y_predictions that come from the X_validate\n",
    "y_pred = clf1.predict(X_validate)\n",
    "\n",
    "# Compare actual y values (from validate) to predicted y_values from the model run on X_validate\n",
    "print(classification_report(y_validate, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f647-939d-41d3-b239-d14a6993a416",
   "metadata": {},
   "source": [
    "### Model - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57337c2c-db46-4820-9c64-6e5b0fac23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest models on train & validate set \n",
    "# by looping through different values for max_depth and min_samples_leaf hyperparameters\n",
    "\n",
    "# create empty list for which to append metrics from each loop\n",
    "scores = []\n",
    "max_value = range(1,21)\n",
    "# create loop for range 1-20\n",
    "for i in max_value:\n",
    "    # set depth & n_samples to value for current loop\n",
    "    depth = i\n",
    "    n_samples = i\n",
    "    # define the model setting hyperparameters to values for current loop\n",
    "    forest = RandomForestClassifier(max_depth=depth, min_samples_leaf=n_samples, random_state=123)\n",
    "    # fit the model on train\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "    # use the model and evaluate performance on train\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)\n",
    "    # use the model and evaluate performance on validate\n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "    # create output of current loop’s hyperparameters and accuracy to append to metrics\n",
    "    output = {\n",
    "        'min_samples_per_leaf': n_samples,\n",
    "        'max_depth': depth,\n",
    "        'train_accuracy': in_sample_accuracy,\n",
    "        'validate_accuracy': out_of_sample_accuracy\n",
    "    }\n",
    "    scores.append(output)\n",
    "# convert metrics list to a dataframe for easy reading\n",
    "df = pd.DataFrame(scores)\n",
    "# add column to assess the difference between train & validate accuracy\n",
    "df['difference'] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0e11d-4214-4823-be77-e7323a0a8621",
   "metadata": {},
   "source": [
    "The Random Forest model that performed the best on train & validate set had max_depth of ____ and min_sample_leaf of _____, with____ accuracy on train, and _____ accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6a08e-d26a-4ae0-827a-791cf6ab0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model setting hyperparameters to values for the best performing model\n",
    "forest = RandomForestClassifier(max_depth=13, min_samples_leaf=13, random_state=123)\n",
    "\n",
    "# fit the model on train\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "# use the model and evaluate performance on train\n",
    "train_accuracy = forest.score(X_train, y_train)\n",
    "# use the model and evaluate performance on validate\n",
    "validate_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "print(f'train_accuracy: {train_accuracy: 2%}')\n",
    "print(f'validate_accuracy: {validate_accuracy: 2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa341b-2ac7-4a06-a034-e831e1719b27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad1db2-0bb9-45b3-996e-08630a2304e6",
   "metadata": {},
   "source": [
    "### Model - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6631a-b353-41b6-8b2f-d6a222e9160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression models on train & validate set by looping through different values for c hyperparameter\n",
    "\n",
    "# create empty list for which to append metrics from each loop\n",
    "metrics = []\n",
    "\n",
    "# create loop for values in list\n",
    "for c in [.001, .005, .01, .05, .1, .5, 1, 5, 10, 50, 100, 500, 1000]:\n",
    "            \n",
    "    # define the model setting hyperparameters to values for current loop\n",
    "    logit = LogisticRegression(C=c)\n",
    "    \n",
    "    # fit the model on train\n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    # use the model and evaluate performance on train\n",
    "    train_accuracy = logit.score(X_train, y_train)\n",
    "    # use the model and evaluate performance on validate\n",
    "    validate_accuracy = logit.score(X_validate, y_validate)\n",
    "    \n",
    "    # create output of current loop's hyperparameters and accuracy to append to metrics\n",
    "    output = {\n",
    "        'C': c,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'validate_accuracy': validate_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "\n",
    "# convert metrics list to a dataframe for easy reading\n",
    "df = pd.DataFrame(metrics)\n",
    "# add column to assess the difference between train & validate accuracy\n",
    "df['difference'] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe947326-6d0f-4fb0-a484-0e2e2f689578",
   "metadata": {},
   "source": [
    "Evaluating the model with the validate data set was done in the function above for comparrison. The Logistic Regression Model that performed best had a c-statistic of _____ with a train accuracy of_____ and validate accuracy of ____ performing ______ as baseline on unseen (validate) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45371d4-1344-441c-8eac-787b0a4dcc9e",
   "metadata": {},
   "source": [
    "### Best Performing Model Applied to Test Data (Unseen Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facaf7a8-ce24-469a-b90b-33c5ba0c3537",
   "metadata": {},
   "source": [
    "All of the best performing models performed equivalent to baseline, at _____ accuracy. The Random Forest model that had max_depth of _____ and min_sample_leaf of _____, performed just slightly higher than baseline with _____ accuracy on train, and _____ accuracy on validate. Due to its preferred performance out of the alternative models, it was applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02128e69-fd24-458f-881a-aee5af933f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest model on train & validate set\n",
    "\n",
    "# define the model setting hyperparameters to values for current loop\n",
    "forest = RandomForestClassifier(max_depth=12, min_samples_leaf=12, random_state=123)\n",
    "\n",
    "# fit the model on train\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "# use the model and evaluate performance on train\n",
    "\n",
    "# use the model and evaluate performance on validate\n",
    "test_accuracy = forest.score(X_test, y_test)\n",
    "\n",
    "print(f'test_accuracy: {test_accuracy: 2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af2bb3-d3f9-4f58-a98e-17a6c6043f99",
   "metadata": {},
   "source": [
    "This model is expected to perform with 93% accuracy in the future on data it has not seen, given no major changes in the data source, which is equivalent to baseline prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed8d8c-c125-4ab5-87ba-bd0378b302bf",
   "metadata": {},
   "source": [
    "========================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cde032-a73c-4a71-b727-1b65c8fc3d5c",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7303ada-42cd-4103-9abf-22ce64cdb704",
   "metadata": {},
   "source": [
    "The goal of this report was to identify drivers of borrower serious delinquency and to build a model that could be used to help borrowers and banking institutions make the best financial decisions.  \n",
    "Through the process of data acquisition, preparation, exploration, and statistical testing, it was determined borrowers more at-risk of serious delinquency are borrowers: \n",
    "\n",
    "- between the ages of 20 and 40\n",
    "- who have an average monthly income of \\\\$4900, which is about \\\\$800 lower than those who are not seriously delinquent  \n",
    "- who have an average debt to income ratio of 16\\%, 13\\% lower than those who are not seriously delinquent\n",
    "- who have an average revolving credit utilization rate of 315\\%, over 250\\% **lower** than those who are not seriously delinquent\n",
    "\n",
    "By using machine learning modeling, predictions to identify serious delinquency were made with 93% accuracy using the best performing model, a Random Forest model with max depth and minimum leaf sample of 12. This is not significantly better than baseline, which is also at 93% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388fc585-d437-4c9a-885a-0af735955c13",
   "metadata": {},
   "source": [
    "### RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4c3fc-eec8-40c8-aa81-5f5d39a6b6b8",
   "metadata": {},
   "source": [
    "- USE THE IDENTIFIED DRIVERS of SERIOUS DELINQUENCY: The data shows age and revolving unsecured line utilization have the largest impact on serious delinquency. These features should be used going forward to predict serious delinquency, as well as a starting point for conducting further analysis.  \n",
    "\n",
    "- BUILD UPON MODEL PERFORMANCE: Although the Polynomial Regression Model, with max depth and minimum leaf sample of 12 does predict serious delinquency with 93% accuracy on unseen data, the model does not perform better than baseline and it could continue to be improved upon through deeper analysis of features and their impact on serious delinquency. See next steps. \n",
    "\n",
    "- CONTINUE to PINPOINT FEATURES DRIVING SERIOUS DELINQUENCY: This report focused on age, monthly income, debt to income ratio, and revolving unsecured line utilization as initial contributors to serious delinquency. It is recommended further analysis and modeling be done with additional features in order to create models with improved performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020f017-8f18-425d-ae6c-0ff5c6efcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEXT STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd529a37-1d50-43a0-b491-471af4c698bc",
   "metadata": {},
   "source": [
    "If given more time, we would like to:\n",
    "\n",
    "- Eliminate more outliers and scale further aspects of the data to produce more accurate modeling/predictions.\n",
    "\n",
    "- Downsample to decrease the quantity of the majority class (no serious delinquency) in order to better see the impact of features on the target.\n",
    "\n",
    "- Identify more precise populations within the data that have serious delinquency. For instance, I would like to investigate:\n",
    "    - the 144 observations that have over 98 occurrences of serious delinquency.\n",
    "    - the relationship between intermediate delinquencies (30 days, 60 days) and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9733345-fbfa-4390-bd70-5e988b9d9d4f",
   "metadata": {},
   "source": [
    "==================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
